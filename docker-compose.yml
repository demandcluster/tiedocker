version: '3.8'

# ============================================================================
# KoboldCpp Docker Compose - Charluv Fork
# ============================================================================
#
# ⚠️  IMPORTANT: This is a Charluv-specialized fork of KoboldCpp
#
# Key differences from official KoboldCpp:
#   - Modified horde integration for Charluv services
#   - Custom API endpoints optimized for Charluv
#
# For official KoboldCpp: https://hub.docker.com/r/koboldai/koboldcpp
# For Charluv: https://charluv.com
#
# Designed for RunPod NVIDIA GPU deployment.
# Requires nvidia-container-toolkit on the host.
# ============================================================================

services:
  koboldcpp:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: koboldcpp
    ports:
      - "5001:5001"
    volumes:
      - ./models:/models
      - ./models/loras:/models/loras
      - ./data:/data
    environment:
      # Model: mount a local file or set MODEL_URL to download on first run
      - KOBOLDCPP_MODEL=/models/model.gguf
      # - KOBOLDCPP_MODEL_URL=https://huggingface.co/.../model.gguf
      # - KOBOLDCPP_MODEL_URL_FILENAME=/models/model.gguf

      - KOBOLDCPP_HOST=0.0.0.0
      - KOBOLDCPP_PORT=5001
      - KOBOLDCPP_CONTEXT_SIZE=8192
      - KOBOLDCPP_THREADS=0
      - KOBOLDCPP_QUIET=true

      # GPU - always CUDA on RunPod
      - KOBOLDCPP_USE_GPU=cuda
      - KOBOLDCPP_GPU_LAYERS=-1
      - CUDA_VISIBLE_DEVICES=0

      # LoRA (optional) - pipe-separate multiple files
      - KOBOLDCPP_LORA=
      - KOBOLDCPP_LORA_MULT=1.0
      - KOBOLDCPP_SDLORA=
      - KOBOLDCPP_SDLORA_MULT=1.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
